{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Load Data\n* Fetch data from csv file and put it into a Pandas DataFrame.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ncovid_data = pd.read_csv('../input/novel-corona-virus-2019-dataset/time_series_covid_19_confirmed.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-10T01:04:35.654674Z","iopub.execute_input":"2022-03-10T01:04:35.655236Z","iopub.status.idle":"2022-03-10T01:04:35.742489Z","shell.execute_reply.started":"2022-03-10T01:04:35.655155Z","shell.execute_reply":"2022-03-10T01:04:35.741183Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation\n* Since we want to predict cases for the US, we will extract the row containing US confirmed covid cases.","metadata":{}},{"cell_type":"code","source":"us_covid_data = covid_data.loc[covid_data['Country/Region'] == 'US']\n\nus_covid_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocess Data\n* We can drop the columns for 'Province/State', 'Country/Region', 'Lat', and 'Long' since we know the data is only for the US and these columns are not needed for prediction.","metadata":{}},{"cell_type":"code","source":"us_covid_data = us_covid_data.drop(columns=['Province/State', 'Country/Region', 'Lat', 'Long'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Since the number of Corona cases gets rather large over time our model's calculations during training may be very slow. We can fix this by using sklearn's MinMaxScaler to rescale our data.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaler.fit(us_covid_data.values.T)\nus_covid_data = scaler.transform(us_covid_data.values.T)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split Into X and y\n* We will set up our X and y in such a way that X[n] will contain the cases for a certain amount of previous days (time_steps), and y[n] will then contain the reading for the next day.\n* This way our model will be trained to predict the number of cases on a certain day based on the trend in the number of cases within the previous time_steps number of days.\n* After some testing, I have found that using the data from the previous 30 days allowed our model to make fairly accurate predictions on the 31st day.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nX, y = [], []\ntime_steps = 30\n\nfor i in range(len(us_covid_data) - time_steps):\n    x = us_covid_data[i:(i+time_steps), 0]\n    X.append(x)\n    y.append(us_covid_data[i+time_steps, 0])\n\nX = np.array(X)\ny = np.array(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Partitioning\n* Must keep the data set in order since we are looking at a chronological timeline of Corona cases, so we can just take the first 80% of the data as our training, and our testing will be the remaining 20%.\n* Also need to reshape the X[n] partitions so our model can process them properly.","metadata":{}},{"cell_type":"code","source":"split = int(len(X) * 0.8)\n\nX_train = X[:split]\nX_test = X[split:]\ny_train = y[:split]\ny_test = y[split:]\n\nX_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\nX_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Architecture\n* We create our model using a reccurent neural network architecture.\n* Model consists of an input layer, followed by three LSTM layers which utilize dropout to prevent our model from overfitting.\n* Output is a Dense layer with a single neuron using ReLU activation function since we are predicting the number of Corona cases, so our output will be a positive number (0, $\\infty$).","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Dropout, Input, LSTM\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import RMSprop\n\nmodel = Sequential()\nmodel.add(Input(shape=(1, time_steps)))\nmodel.add(LSTM(48, return_sequences=True))\nmodel.add(Dropout(0.4))\nmodel.add(LSTM(48, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(48))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='relu'))\n\n\nmodel.compile(loss = 'mean_squared_error',\n              optimizer = RMSprop(),\n              metrics = ['mean_squared_error'])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the Model\n* Now we can train our model using 20% of the training data as our validation set.\n* Model will utilize the ReduceLROnPlateau to lower our learning rate any time our validation MSE plateaus for three epochs for best accuracy.","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import ReduceLROnPlateau\n\nbatchsize = 100\nepochs =  100\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_mean_squared_error', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=1e-10)\n\nhistory = model.fit(X_train,\n                    y_train,\n                    batch_size=batchsize,\n                    epochs=epochs,\n                    validation_split=0.2,\n                    shuffle=False,\n                    callbacks=[learning_rate_reduction])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Plot the model's loss and MSE values throughout training.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['train', 'val'])\nplt.show()\n\nplt.plot(history.history['mean_squared_error'])\nplt.plot(history.history['val_mean_squared_error'])\nplt.title('Model Error')\nplt.ylabel('Mean Squared Error')\nplt.xlabel('Epochs')\nplt.legend(['train', 'val'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot Model Predictions\n* In order to see the accuracy of our model, we first use it to predict the output of our X_test data.\n* We then rescale our prediction and y_test data back to the original bounds of the data set in order to accurately plot their values.\n* Finally, we can plot the actual Covid cases compared to our predicted Covid cases to see the overall accuracy of our model.","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(X_test)\ny_pred = scaler.inverse_transform(y_pred)\ny_test = scaler.inverse_transform(y_test.reshape(-1,1))\n\nplt.plot(y_pred, color='red')\nplt.plot(y_test, color='blue')\nplt.title('Actual vs. Predicted Covid Cases (Test Data)')\nplt.ylabel('Number of Cases')\nplt.xlabel('Day')\nplt.legend(['predicted', 'actual'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}